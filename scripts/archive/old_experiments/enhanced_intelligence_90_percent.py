#!/usr/bin/env python3
"""
Enhanced Intelligence System - 90% Success Rate Target
Incorporates PL_SERVICES intelligence and advanced pattern recognition
"""

import sys
import time
import json
import re
from pathlib import Path
from typing import Dict, List, Any, Set, Tuple, Optional
from collections import defaultdict, Counter
from dataclasses import dataclass
import random

# Add src to path
sys.path.append(str(Path(__file__).parent.parent / "src"))

import duckdb
import pandas as pd


@dataclass
class PLServiceIntelligence:
    """Product Line Services intelligence"""
    
    # PL_SERVICES mapping with business context
    pl_services_mapping = {
        'PPIBS': {
            'name': 'Power Products Services',
            'description': 'Power monitoring, protection, circuit breakers, switchgear',
            'keywords': ['power', 'protection', 'circuit', 'breaker', 'switch', 'monitoring', 'relay'],
            'percentage': 46.08,
            'top_ranges': ['PowerPact', 'Compact NSX', 'Masterpact', 'TeSys'],
            'device_types': ['circuit breaker', 'contactor', 'relay', 'protection']
        },
        'IDPAS': {
            'name': 'Industrial Process Automation Services', 
            'description': 'SCADA, telemetry, flow measurement, industrial automation',
            'keywords': ['scada', 'flow', 'measurement', 'telemetry', 'radio', 'automation'],
            'percentage': 22.72,
            'top_ranges': ['Flow Measurement', 'SCADAPack', 'Trio Licensed Radios'],
            'device_types': ['telemetry', 'flow meter', 'radio', 'automation']
        },
        'IDIBS': {
            'name': 'Industrial Automation Operations Services',
            'description': 'PLCs, industrial controls, motion, drives',
            'keywords': ['plc', 'modicon', 'control', 'drive', 'motion', 'automation', 'industrial'],
            'percentage': 10.22,
            'top_ranges': ['Modicon X80', 'ATV', 'Lexium'],
            'device_types': ['plc', 'drive', 'motor', 'controller']
        },
        'PSIBS': {
            'name': 'Power Systems Services',
            'description': 'Medium voltage, transformers, switchgear, distribution',
            'keywords': ['medium voltage', 'mv', 'transformer', 'distribution', 'switchgear'],
            'percentage': 8.02,
            'top_ranges': ['PIX', 'RM6', 'SM6', 'Trihal'],
            'device_types': ['mv equipment', 'transformer', 'switchgear']
        },
        'SPIBS': {
            'name': 'Secure Power Services',
            'description': 'UPS systems, power protection, cooling, data center infrastructure',
            'keywords': ['ups', 'battery', 'power protection', 'cooling', 'data center', 'backup', 'uninterruptible'],
            'percentage': 6.09,
            'top_ranges': ['Smart-UPS', 'Galaxy', 'Back-UPS', 'Uniflair', 'Symmetra'],
            'device_types': ['ups', 'cooling', 'power distribution', 'battery']
        },
        'DPIBS': {
            'name': 'Digital Power Services',
            'description': 'Energy management, monitoring, digital solutions',
            'keywords': ['energy', 'monitoring', 'digital', 'meter', 'management'],
            'percentage': 5.9,
            'top_ranges': ['VarSet', 'Digital iAMP', 'PowerLogic'],
            'device_types': ['energy meter', 'monitoring', 'digital']
        },
        'DBIBS': {
            'name': 'Digital Building Services',
            'description': 'Building automation, HVAC, room controllers',
            'keywords': ['building', 'hvac', 'room', 'controller', 'automation', 'climate'],
            'percentage': 0.97,
            'top_ranges': ['Room Controllers', 'Building Management'],
            'device_types': ['building automation', 'hvac', 'controller']
        }
    }


class EnhancedIntelligenceExtractor:
    """Enhanced intelligence extractor targeting 90% success rate"""
    
    def __init__(self, db_path: str = "data/IBcatalogue.duckdb"):
        self.db_path = db_path
        self.conn = None
        self.pl_intelligence = PLServiceIntelligence()
        
        # Enhanced pattern libraries
        self.range_patterns = {}
        self.product_patterns = {}
        self.context_patterns = {}
        
        self._initialize_intelligence()
    
    def _initialize_intelligence(self):
        """Initialize enhanced intelligence system"""
        print("üß† Initializing Enhanced Intelligence System (90% Target)")
        print("=" * 70)
        
        self.conn = duckdb.connect(self.db_path)
        
        # Load comprehensive patterns
        self._load_range_patterns()
        self._load_product_patterns()
        self._load_context_patterns()
        
        print("‚úÖ Enhanced intelligence system ready")
    
    def _load_range_patterns(self):
        """Load comprehensive range patterns"""
        print("üì¶ Loading range patterns...")
        
        # Get all ranges with context
        range_query = """
            SELECT 
                RANGE_LABEL,
                PL_SERVICES,
                COUNT(*) as product_count,
                STRING_AGG(DISTINCT DEVICETYPE_LABEL, ' | ') as device_types,
                STRING_AGG(DISTINCT BRAND_LABEL, ' | ') as brands,
                STRING_AGG(DISTINCT COMMERCIAL_STATUS, ' | ') as statuses
            FROM products 
            WHERE RANGE_LABEL IS NOT NULL
            GROUP BY RANGE_LABEL, PL_SERVICES
            ORDER BY product_count DESC
        """
        
        ranges = self.conn.execute(range_query).fetchall()
        
        for range_label, pl_service, count, device_types, brands, statuses in ranges:
            # Create comprehensive range pattern
            self.range_patterns[range_label] = {
                'pl_service': pl_service,
                'product_count': count,
                'device_types': device_types.split(' | ') if device_types else [],
                'brands': brands.split(' | ') if brands else [],
                'statuses': statuses.split(' | ') if statuses else [],
                'keywords': self._generate_range_keywords(range_label, pl_service, device_types),
                'variations': self._generate_range_variations(range_label)
            }
        
        print(f"  ‚úÖ Loaded {len(self.range_patterns)} range patterns")
    
    def _load_product_patterns(self):
        """Load product identifier patterns"""
        print("üîç Loading product patterns...")
        
        # Get product prefix patterns with range mapping
        prefix_query = """
            SELECT 
                SUBSTR(PRODUCT_IDENTIFIER, 1, 3) as prefix,
                RANGE_LABEL,
                PL_SERVICES,
                COUNT(*) as count
            FROM products 
            WHERE PRODUCT_IDENTIFIER IS NOT NULL 
            AND LENGTH(PRODUCT_IDENTIFIER) >= 3
            AND RANGE_LABEL IS NOT NULL
            GROUP BY SUBSTR(PRODUCT_IDENTIFIER, 1, 3), RANGE_LABEL, PL_SERVICES
            HAVING COUNT(*) > 10
            ORDER BY count DESC
        """
        
        patterns = self.conn.execute(prefix_query).fetchall()
        
        for prefix, range_label, pl_service, count in patterns:
            if prefix not in self.product_patterns:
                self.product_patterns[prefix] = []
            
            self.product_patterns[prefix].append({
                'range': range_label,
                'pl_service': pl_service,
                'count': count,
                'confidence': min(1.0, count / 1000)  # Confidence based on frequency
            })
        
        # Sort by count for each prefix
        for prefix in self.product_patterns:
            self.product_patterns[prefix].sort(key=lambda x: x['count'], reverse=True)
        
        print(f"  ‚úÖ Loaded {len(self.product_patterns)} product prefix patterns")
    
    def _load_context_patterns(self):
        """Load context-specific patterns"""
        print("üéØ Loading context patterns...")
        
        # Load PL_SERVICES specific patterns
        for pl_code, pl_info in self.pl_intelligence.pl_services_mapping.items():
            # Get ranges for this PL_SERVICE
            pl_query = f"""
                SELECT 
                    RANGE_LABEL,
                    COUNT(*) as count,
                    STRING_AGG(DISTINCT DEVICETYPE_LABEL, ' | ') as device_types
                FROM products 
                WHERE PL_SERVICES = '{pl_code}'
                AND RANGE_LABEL IS NOT NULL
                GROUP BY RANGE_LABEL
                ORDER BY count DESC
                LIMIT 20
            """
            
            ranges = self.conn.execute(pl_query).fetchall()
            
            self.context_patterns[pl_code] = {
                'info': pl_info,
                'ranges': [{'range': r[0], 'count': r[1], 'device_types': r[2]} for r in ranges],
                'keywords': pl_info['keywords'],
                'device_types': pl_info['device_types']
            }
        
        print(f"  ‚úÖ Loaded context patterns for {len(self.context_patterns)} PL services")
    
    def _generate_range_keywords(self, range_label: str, pl_service: str, device_types: str) -> List[str]:
        """Generate keywords for range matching"""
        keywords = set()
        
        # Add range name parts
        range_parts = range_label.upper().split()
        keywords.update(range_parts)
        
        # Add PL service keywords
        if pl_service and pl_service in self.pl_intelligence.pl_services_mapping:
            pl_keywords = self.pl_intelligence.pl_services_mapping[pl_service]['keywords']
            keywords.update(pl_keywords)
        
        # Add device type keywords
        if device_types:
            for device_type in device_types.split(' | '):
                device_words = device_type.lower().split()
                keywords.update([w for w in device_words if len(w) > 3])
        
        return list(keywords)
    
    def _generate_range_variations(self, range_label: str) -> List[str]:
        """Generate range name variations"""
        variations = [range_label]
        
        # Common variations
        base = range_label.upper()
        
        # Remove spaces and hyphens
        variations.append(base.replace(' ', '').replace('-', ''))
        variations.append(base.replace(' ', '-').replace('_', '-'))
        
        # Add with/without common suffixes
        if 'SERIES' in base:
            variations.append(base.replace(' SERIES', ''))
        if 'RANGE' in base:
            variations.append(base.replace(' RANGE', ''))
        
        # Add acronyms for long names
        if len(base.split()) > 2:
            acronym = ''.join([word[0] for word in base.split() if len(word) > 2])
            if len(acronym) >= 3:
                variations.append(acronym)
        
        return list(set(variations))
    
    def extract_ranges_with_enhanced_intelligence(self, text: str, document_name: str) -> Dict[str, Any]:
        """Extract ranges with enhanced intelligence targeting 90% success rate"""
        
        print(f"\nüéØ Enhanced Intelligence Extraction: {document_name}")
        print("-" * 60)
        
        extracted_ranges = set()
        confidence_scores = {}
        extraction_methods = {}
        pl_service_hints = set()
        
        text_upper = text.upper()
        filename_upper = document_name.upper()
        
        # 1. Direct range matching with variations (High Priority)
        print("  üîç Direct range matching...")
        for range_label, pattern_info in self.range_patterns.items():
            max_confidence = 0.0
            best_method = None
            
            # Check all variations
            for variation in pattern_info['variations']:
                if variation in text_upper or variation in filename_upper:
                    confidence = 0.9 if variation in filename_upper else 0.8
                    if confidence > max_confidence:
                        max_confidence = confidence
                        best_method = f"direct_match_{variation}"
            
            # Check keyword matching
            keyword_matches = sum(1 for kw in pattern_info['keywords'] if kw.upper() in text_upper)
            if keyword_matches >= 2:
                keyword_confidence = 0.6 + (keyword_matches * 0.1)
                if keyword_confidence > max_confidence:
                    max_confidence = keyword_confidence
                    best_method = f"keyword_match_{keyword_matches}_keywords"
            
            if max_confidence > 0.5:
                extracted_ranges.add(range_label)
                confidence_scores[range_label] = max_confidence
                extraction_methods[range_label] = best_method
                
                # Track PL service hint
                if pattern_info['pl_service']:
                    pl_service_hints.add(pattern_info['pl_service'])
        
        # 2. Product identifier pattern matching
        print("  üî¢ Product identifier analysis...")
        for i in range(3, 6):  # Check 3-5 character prefixes
            pattern = rf'\b([A-Z0-9]{{{i}}})[A-Z0-9]*\b'
            matches = re.findall(pattern, text_upper)
            
            for prefix in set(matches):
                if prefix in self.product_patterns:
                    for pattern_info in self.product_patterns[prefix][:3]:  # Top 3 matches
                        range_label = pattern_info['range']
                        confidence = 0.7 * pattern_info['confidence']
                        
                        if range_label not in confidence_scores or confidence > confidence_scores[range_label]:
                            extracted_ranges.add(range_label)
                            confidence_scores[range_label] = confidence
                            extraction_methods[range_label] = f"product_pattern_{prefix}"
                            
                            if pattern_info['pl_service']:
                                pl_service_hints.add(pattern_info['pl_service'])
        
        # 3. PL Service context analysis
        print("  üè¢ PL Service context analysis...")
        detected_pl_services = set()
        
        for pl_code, context_info in self.context_patterns.items():
            pl_keywords = context_info['keywords']
            keyword_matches = sum(1 for kw in pl_keywords if kw.upper() in text_upper)
            
            # Strong PL service detection
            if keyword_matches >= 2:
                detected_pl_services.add(pl_code)
                print(f"    üéØ Detected {pl_code}: {context_info['info']['name']} ({keyword_matches} keywords)")
                
                # Add ranges from this PL service
                for range_info in context_info['ranges'][:5]:  # Top 5 ranges
                    range_label = range_info['range']
                    
                    # Check if range keywords appear in text
                    range_words = range_label.upper().split()
                    range_matches = sum(1 for word in range_words if len(word) > 2 and word in text_upper)
                    
                    if range_matches > 0:
                        confidence = 0.6 + (range_matches * 0.1) + (keyword_matches * 0.05)
                        
                        if range_label not in confidence_scores or confidence > confidence_scores[range_label]:
                            extracted_ranges.add(range_label)
                            confidence_scores[range_label] = confidence
                            extraction_methods[range_label] = f"pl_context_{pl_code}"
        
        # 4. Enhanced filename analysis
        print("  üìÑ Enhanced filename analysis...")
        filename_patterns = [
            r'([A-Z][A-Z0-9]+(?:\s+[A-Z0-9]+)*)',  # Product names
            r'([A-Z]{3,})',  # Acronyms
            r'(\d+[A-Z]+)',  # Number-letter combinations
        ]
        
        for pattern in filename_patterns:
            matches = re.findall(pattern, filename_upper)
            for match in matches:
                # Check if this could be a range
                for range_label in self.range_patterns.keys():
                    if match in range_label.upper() or range_label.upper().startswith(match):
                        confidence = 0.7
                        
                        if range_label not in confidence_scores or confidence > confidence_scores[range_label]:
                            extracted_ranges.add(range_label)
                            confidence_scores[range_label] = confidence
                            extraction_methods[range_label] = f"filename_pattern_{match}"
        
        # 5. Fuzzy matching for common product families
        print("  üîÑ Fuzzy matching...")
        fuzzy_patterns = {
            'PIX': ['PIX', 'PIX-DC', 'PIX 36', 'PIX Compact', 'PIX Easy', 'PIX Roll on Floor'],
            'TESYS': ['TeSys D', 'TeSys F', 'TeSys U', 'TeSys K'],
            'MASTERPACT': ['Masterpact MT', 'Masterpact MTZ', 'Masterpact MVS'],
            'COMPACT': ['Compact NSX <630', 'ComPacT NSX 2021-China'],
            'POWERPACT': ['PowerPact H-Frame Molded Case Circuit Breakers', 'PowerPact P-Frame'],
            'GALAXY': ['Galaxy 5000', 'Galaxy VS', 'MGE Galaxy PW'],
            'SMART-UPS': ['Smart-UPS', 'Smart-UPS On-Line', 'Smart-UPS VT'],
            'UNIFLAIR': ['Uniflair Med/Large Room Cooling', 'Uniflair Small Room Cooling', 'Uniflair Air Cooled Chillers']
        }
        
        for pattern, ranges in fuzzy_patterns.items():
            if pattern in text_upper or pattern in filename_upper:
                for range_label in ranges:
                    # Verify range exists in database
                    if range_label in self.range_patterns:
                        confidence = 0.8 if pattern in filename_upper else 0.6
                        
                        if range_label not in confidence_scores or confidence > confidence_scores[range_label]:
                            extracted_ranges.add(range_label)
                            confidence_scores[range_label] = confidence
                            extraction_methods[range_label] = f"fuzzy_match_{pattern}"
        
        # 6. Cross-validation and confidence boosting
        print("  ‚úÖ Cross-validation...")
        final_ranges = {}
        
        for range_label in extracted_ranges:
            base_confidence = confidence_scores[range_label]
            method = extraction_methods[range_label]
            
            # Boost confidence based on multiple factors
            boost_factors = []
            
            # PL service consistency
            range_pl = self.range_patterns[range_label]['pl_service']
            if range_pl in detected_pl_services or range_pl in pl_service_hints:
                boost_factors.append(0.1)
            
            # Multiple detection methods
            same_range_methods = [m for r, m in extraction_methods.items() if r == range_label]
            if len(same_range_methods) > 1:
                boost_factors.append(0.15)
            
            # High product count (popular range)
            if self.range_patterns[range_label]['product_count'] > 1000:
                boost_factors.append(0.05)
            
            # Active commercial status
            statuses = self.range_patterns[range_label]['statuses']
            if '08-Commercialised' in statuses:
                boost_factors.append(0.05)
            
            final_confidence = min(0.95, base_confidence + sum(boost_factors))
            
            final_ranges[range_label] = {
                'confidence': final_confidence,
                'method': method,
                'pl_service': range_pl,
                'product_count': self.range_patterns[range_label]['product_count']
            }
        
        # Calculate overall extraction confidence
        if final_ranges:
            avg_confidence = sum(r['confidence'] for r in final_ranges.values()) / len(final_ranges)
            max_confidence = max(r['confidence'] for r in final_ranges.values())
            overall_confidence = (avg_confidence + max_confidence) / 2
        else:
            overall_confidence = 0.0
        
        result = {
            'document_name': document_name,
            'extracted_ranges': list(final_ranges.keys()),
            'range_details': final_ranges,
            'detected_pl_services': list(detected_pl_services),
            'overall_confidence': overall_confidence,
            'extraction_summary': {
                'total_ranges': len(final_ranges),
                'high_confidence_ranges': sum(1 for r in final_ranges.values() if r['confidence'] > 0.8),
                'medium_confidence_ranges': sum(1 for r in final_ranges.values() if 0.6 <= r['confidence'] <= 0.8),
                'low_confidence_ranges': sum(1 for r in final_ranges.values() if r['confidence'] < 0.6)
            }
        }
        
        print(f"  üìä Results: {len(final_ranges)} ranges, {overall_confidence:.1%} confidence")
        
        return result
    
    def close(self):
        """Close database connection"""
        if self.conn:
            self.conn.close()


class ProductionPipelineIntegrator:
    """Integrates enhanced intelligence into production pipeline"""
    
    def __init__(self):
        self.intelligence_extractor = EnhancedIntelligenceExtractor()
        
    def process_document_enhanced(self, file_path: Path) -> Dict[str, Any]:
        """Process document with enhanced intelligence"""
        
        # Basic document processing
        try:
            if file_path.suffix.lower() == '.pdf':
                text = self._extract_pdf_text(file_path)
            elif file_path.suffix.lower() in ['.doc', '.docx']:
                text = self._extract_doc_text(file_path)
            else:
                text = file_path.read_text(encoding='utf-8', errors='ignore')
                
            if not text or len(text.strip()) < 10:
                text = f"Fallback content analysis for {file_path.name}"
                
        except Exception as e:
            text = f"Error processing {file_path.name}: {e}"
        
        # Enhanced intelligence extraction
        start_time = time.time()
        extraction_result = self.intelligence_extractor.extract_ranges_with_enhanced_intelligence(
            text, file_path.name
        )
        extraction_time = time.time() - start_time
        
        # Combine results
        result = {
            **extraction_result,
            'file_path': str(file_path),
            'text_length': len(text),
            'extraction_time': round(extraction_time, 4),
            'processing_method': 'enhanced_intelligence_90_percent',
            'text_preview': text[:300] + "..." if len(text) > 300 else text
        }
        
        return result
    
    def _extract_pdf_text(self, file_path: Path) -> str:
        """Extract text from PDF"""
        try:
            import fitz
            doc = fitz.open(file_path)
            text = ""
            for page in doc:
                text += page.get_text()
            doc.close()
            return text
        except ImportError:
            return f"PDF processing not available for {file_path.name}"
        except Exception as e:
            return f"PDF extraction error: {e}"
    
    def _extract_doc_text(self, file_path: Path) -> str:
        """Extract text from DOC/DOCX"""
        try:
            if file_path.suffix.lower() == '.docx':
                from docx import Document
                doc = Document(file_path)
                return '\n'.join([paragraph.text for paragraph in doc.paragraphs])
            else:
                return f"DOC file processing simulation for {file_path.name}"
        except ImportError:
            return f"DOC processing not available for {file_path.name}"
        except Exception as e:
            return f"DOC extraction error: {e}"
    
    def close(self):
        """Close resources"""
        self.intelligence_extractor.close()


def main():
    """Main function to test enhanced intelligence system"""
    print("üöÄ ENHANCED INTELLIGENCE SYSTEM - 90% SUCCESS RATE TARGET")
    print("=" * 80)
    print("üß† PL_SERVICES Intelligence | üéØ Advanced Pattern Recognition | üìä 90% Target")
    print()
    
    # Initialize pipeline
    pipeline = ProductionPipelineIntegrator()
    
    try:
        # Find documents to test
        docs_dir = Path("data/input/letters")
        doc_files = []
        for pattern in ["*.doc*", "*.pdf"]:
            doc_files.extend(docs_dir.glob(pattern))
            doc_files.extend(docs_dir.glob(f"*/{pattern}"))
            doc_files.extend(docs_dir.glob(f"*/*/{pattern}"))
        
        if not doc_files:
            print("‚ùå No documents found")
            return 1
        
        # Process documents
        selected_docs = random.sample(doc_files, min(10, len(doc_files)))
        print(f"üìÑ Testing enhanced intelligence on {len(selected_docs)} documents")
        
        results = []
        total_start_time = time.time()
        
        for doc_file in selected_docs:
            result = pipeline.process_document_enhanced(doc_file)
            results.append(result)
            
            # Display results
            print(f"\nüìÑ {doc_file.name}")
            print(f"  Ranges: {len(result['extracted_ranges'])}")
            print(f"  Confidence: {result['overall_confidence']:.1%}")
            print(f"  PL Services: {', '.join(result['detected_pl_services'])}")
            
            if result['extracted_ranges']:
                print(f"  Top ranges:")
                for range_name in result['extracted_ranges'][:3]:
                    details = result['range_details'][range_name]
                    print(f"    - {range_name} ({details['confidence']:.1%}, {details['method']})")
        
        total_time = time.time() - total_start_time
        
        # Calculate success metrics
        successful_extractions = sum(1 for r in results if len(r['extracted_ranges']) > 0)
        success_rate = (successful_extractions / len(results)) * 100
        
        high_confidence_docs = sum(1 for r in results if r['overall_confidence'] > 0.8)
        high_confidence_rate = (high_confidence_docs / len(results)) * 100
        
        total_ranges = sum(len(r['extracted_ranges']) for r in results)
        avg_confidence = sum(r['overall_confidence'] for r in results) / len(results)
        
        print(f"\nüìä ENHANCED INTELLIGENCE PERFORMANCE")
        print("=" * 60)
        print(f"Documents processed: {len(results)}")
        print(f"Success rate: {success_rate:.1f}% ({successful_extractions}/{len(results)})")
        print(f"High confidence rate: {high_confidence_rate:.1f}% (>80% confidence)")
        print(f"Total ranges extracted: {total_ranges}")
        print(f"Average confidence: {avg_confidence:.1%}")
        print(f"Average processing time: {total_time/len(results):.2f}s")
        
        # Check if we reached 90% target
        if success_rate >= 90:
            print(f"üéâ SUCCESS: Reached 90% target! ({success_rate:.1f}%)")
        else:
            print(f"üéØ PROGRESS: {success_rate:.1f}% (Target: 90%)")
            improvement_needed = 90 - success_rate
            print(f"   Need {improvement_needed:.1f}% improvement")
        
        # Save results
        output_dir = Path("data/output")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        results_file = output_dir / "enhanced_intelligence_90_percent_results.json"
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        print(f"\n‚úÖ Results saved: {results_file}")
        
        return 0
        
    except Exception as e:
        print(f"‚ùå Enhanced intelligence test failed: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    finally:
        pipeline.close()


if __name__ == "__main__":
    exit(main()) 