# Readme

**Version: 20.2*Author: Alexandre Huther**  
**Date:202507-16**

[![Python3.9ttps://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-00g)](https://github.com/psf/black)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![xAI SDK](https://img.shields.io/badge/xAI-Official%20-green.svg)](https://docs.x.ai/)
[![PostgreSQL](https://img.shields.io/badge/Database-PostgreSQL-blue.svg)](https://www.postgresql.org/)
[![Success Rate](https://img.shields.io/badge/Success%20Rate-10025brightgreen.svg)](https://github.com/humananalog/se-letters)

A comprehensive automated pipeline for processing Schneider Electric obsolescence letters and matching them to the **IBcatalogue.xlsx master referential** containing **342lectrical products** using advanced AI/ML techniques with the **official xAI SDK** and comprehensive metadata extraction.

**Current Version: 2.2.1stgreSQL Migration Complete**

## üöÄ Enhanced Features

### üóÑÔ∏è PostgreSQL Database Integration
- **Production Database**: Migrated from DuckDB to PostgreSQL for enterprise scalability
- **Concurrent Access**: Eliminates database lock conflicts for multi-user environments
- **ACID Compliance**: Full transaction support with rollback capabilities
- **Connection Pooling**: Optimized database connections for high-performance processing
- **Schema Migration**: Complete data migration with 100% integrity preservation

### ü§ñ Official xAI SDK Integration
- **Official xAI SDK**: Uses the official `xai-sdk` package for reliable API communication
- **Grok-3 Latest**: Powered by the latest Grok-3 model for maximum accuracy
- **100uccess Rate**: Reliable document processing with comprehensive error handling
- **95% Confidence**: Average confidence score for extracted product information
- **Sub-second Responses**: Optimized API calls with proper retry logic

### üîç Multi-Dimensional Semantic Extraction
- **Zero Assumptions**: Discovers product ranges without prior knowledge
- **6-Dimensional Search**: Range, subrange, device type, brand, PL services, technical specs
- **Search Space Refinement**: Up to 99.6tion for precision targeting
- **Multi-format Support**: PDF, DOCX, DOC with LibreOffice integration and OCR
- **Robust Processing**: Multiple fallback strategies for reliable extraction
- **Production Ready**: Enhanced from previous 40% failure rate to100# ü§ñ Comprehensive AI Metadata Extraction
- **Complete IBcatalogue Coverage**: Extracts ALL metadata corresponding to 29 IBcatalogue fields
- **Discovery-Based**: Finds whatever product ranges are actually mentioned in documents
- **No Hallucination**: Only extracts explicitly stated information
- **Business Intelligence**: Captures customer impact, service details, migration guidance
- **Confidence Scoring**: Each product extraction includes confidence metrics

### üìä IBcatalogue Master Referential Integration
- **342oducts**: Complete Schneider Electric product database
- **29 Data Fields**: Technical specs, commercial status, service information
- **Global Coverage**: All brands, business units, and geographic regions
- **Complete Lifecycle Data**: Production, commercialization, and service dates

### üîç Enhanced Vector Search Engine (Phase 3)
- **Hierarchical Vector Spaces**: Multi-level search across PL services, brands, and business units
- **PPIBS Gap Analysis**: Identifies157,713red PPIBS products (460.1 of total)
- **Intelligent Search Strategies**: 4 specialized search methods for optimal product discovery
- **Production-Ready**: Sub-second search times with 95%+ confidence scoring

### üìã Comprehensive Product Export
- **Multi-sheet Excel**: Complete product lists with all IBcatalogue details
- **Business Analysis**: Status breakdowns, obsolescence timelines, service impact
- **Range-specific Exports**: Dedicated sheets for each discovered product range
- **CSV Support**: Lightweight exports for data analysis

### üéØ Advanced Matching & Analysis
- **Semantic Search**: FAISS-based similarity search across 342,229 products
- **Complete Product Discovery**: Finds ALL products where letters apply
- **Business Context**: Customer impact assessment and migration planning
- **Quality Assurance**: Confidence scoring and limitation reporting

### üåê Production Web Application
- **Next.js Frontend**: Modern, responsive web interface
- **Real-time Processing**: Live document processing with progress tracking
- **Document Management**: Upload, process, and view results
- **API Integration**: RESTful APIs for all pipeline operations
- **Production Ready**: Deployed and tested with actual documents

## üìã Requirements

- Python 3.9 or higher
- PostgreSQL 15roduction database)
- xAI API key (for Grok-3ess)
- LibreOffice (for DOC/DOCX processing)
- Tesseract OCR (for image text extraction)
-4M (for IBcatalogue processing)
- Node.js 18+ (for web application)

## üõ†Ô∏è Installation

### 1. Clone the repository
```bash
git clone https://github.com/humananalog/se-letters.git
cd se-letters
```

### 2. Create a virtual environment
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

###4ystem dependencies
```bash
# Ubuntu/Debian
sudo apt-get install postgresql postgresql-contrib libreoffice tesseract-ocr

# macOS
brew install postgresql@15 libreoffice tesseract

# Windows
# Download and install PostgreSQL, LibreOffice and Tesseract manually
```

### 5. Set up PostgreSQL database
```bash
# Create database and user
sudo -u postgres createdb se_letters
sudo -u postgres createuser ahuther
sudo -u postgres psql -c "ALTER USER ahuther WITH PASSWORDpassword';
sudo -u postgres psql -c GRANT ALL PRIVILEGES ON DATABASE se_letters TO ahuther;"

# Run schema migration
psql -U ahuther -h localhost -d se_letters -f scripts/migrate_schema_to_postgresql.sql
```

### 6. Set up pre-commit hooks (optional)
```bash
pre-commit install
```

## ‚öôÔ∏è Configuration

### 1. Environment Variables
Set your database and API credentials:

```bash
export DATABASE_URL="postgresql://ahuther:password@localhost:5432/se_letters"
export XAI_API_KEY=your_xai_api_key_here
```

Or create a `.env` file:
```env
DATABASE_URL=postgresql://ahuther:password@localhost:5432se_letters
XAI_API_KEY=your_xai_api_key_here
```

### 2. Configuration File
The main configuration is in `config/config.yaml`. Key settings include:

```yaml
# Database Configuration
database:
  postgresql:
    host: localhost
    port: 5432
    database: se_letters
    user: ahuther
    password: password
    pool_size:20   connection_timeout: 30
    idle_timeout: 300

# API Configuration
api:
  xai:
    base_url:https://api.x.ai/v1
    model: "grok-3-latest"
    # api_key loaded from XAI_API_KEY environment variable
    max_tokens: 4096
    temperature:00.1
    timeout: 30

# Data Configuration
data:
  excel_file: "data/input/letters/IBcatalogue.xlsx
  excel_sheet: "OIC_out"
  
  # IBcatalogue column mappings (29 fields)
  columns:
    product_id:PRODUCT_IDENTIFIER"
    range_name:RANGE_LABEL    description: "PRODUCT_DESCRIPTION
    # ... all29ields mapped
```

## üöÄ Usage

### Web Application
Start the web application for interactive document processing:

```bash
cd webapp
npm install
npm run dev
```

Visit `http://localhost:30 to access the web interface.

### Production Pipeline Processing
Process documents through the production pipeline:

```bash
# Process any obsolescence letter
python scripts/production_pipeline_runner.py path/to/document.pdf

# Process with comprehensive analysis
python scripts/production_pipeline_runner.py --comprehensive path/to/document.pdf
```

### API Endpoints
```bash
# Test document processing
curl -X POST http://localhost:3000/api/pipeline/test-process \
  -H "Content-Type: application/json" \
  -d{"documentId: est-doc-1,forceReprocess: true}'

# Check pipeline status
curl -X GET http://localhost:3000pi/pipeline/status

# Get letters from database
curl -X GET http://localhost:3000/api/letters
```

### Python API

```python
from se_letters.core.config import get_config
from se_letters.services import XAIService, PostgreSQLProductionPipelineService

# Initialize services
config = get_config()
xai_service = XAIService(config)
pipeline = PostgreSQLProductionPipelineService()

# Process document through production pipeline
result = pipeline.process_document(document_path)

# Extract metadata using xAI service
metadata = xai_service.extract_comprehensive_metadata(
    text=document_text, 
    document_name=document_name
)

print(f"Success: {result.success}")
print(f"Confidence: {result.confidence_score}")
print(fProducts found: {len(result.validation_result.product_ranges)}")
```

## üìä Recent Processing Results

### SEPAM2040_Notice.pdf
Recent processing of SEPAM obsolescence letter:
- **Processing Time**:126nds
- **Confidence Score**: 95%
- **Products Extracted**: 5 unique product ranges
  - MiCOM P20- SEPAM20 - SEPAM 40  - MiCOM P521
  - PowerLogic P5L
- **Database Storage**: Successfully stored in PostgreSQL with confidence scoring
- **Success Rate**: 100%

### PIX2B_Phase_out_Letter.pdf
Processing of PIX range obsolescence letter:
- **Processing Time**:108nds
- **Confidence Score**: 95%
- **Products Extracted**: PIX Double Bus Bar (PIX 2B) range
- **Database Storage**: Successfully stored and indexed in PostgreSQL
- **Deduplication**: Correctly identifies already processed documents

### Performance Metrics
- **Overall Success Rate**: 100%
- **Average Processing Time**: ~12 seconds per document
- **Average Confidence**: 95%
- **API Response Time**: Sub-second validation
- **Database Operations**: Optimized with PostgreSQL indexing and connection pooling

## üìÅ Enhanced Project Structure

```
se-letters/
‚îú‚îÄ‚îÄ src/se_letters/           # Main source code
‚îÇ   ‚îú‚îÄ‚îÄ core/                 # Core business logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py         # Enhanced configuration with env vars
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ exceptions.py     # Custom exceptions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ postgresql_database.py  # PostgreSQL database layer
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pipeline.py       # Main pipeline orchestration
‚îÇ   ‚îú‚îÄ‚îÄ models/               # Data models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ document.py       # Document processing models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ letter.py         # Letter and comprehensive metadata
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ product.py        # IBcatalogue product models
‚îÇ   ‚îú‚îÄ‚îÄ services/             # Production services
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ postgresql_production_pipeline_service.py  # PostgreSQL pipeline
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ postgresql_letter_database_service.py      # PostgreSQL database service
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ document_processor.py           # Document processing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embedding_service.py            # FAISS semantic search
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ excel_service.py                # IBcatalogue integration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ xai_service.py                  # Official xAI SDK integration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ preview_service.py              # Document preview
‚îÇ   ‚îî‚îÄ‚îÄ utils/                # Utility functions
‚îú‚îÄ‚îÄ webapp/                   # Next.js web application
‚îÇ   ‚îú‚îÄ‚îÄ src/app/api/         # API routes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipeline/        # Pipeline endpoints
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ letters/         # Letter management
‚îÇ   ‚îú‚îÄ‚îÄ src/components/      # React components
‚îÇ   ‚îî‚îÄ‚îÄ src/types/           # TypeScript definitions
‚îú‚îÄ‚îÄ scripts/                  # Production scripts
‚îÇ   ‚îú‚îÄ‚îÄ production_pipeline_runner.py      # Main pipeline runner
‚îÇ   ‚îú‚îÄ‚îÄ migrate_schema_to_postgresql.sql   # PostgreSQL schema
‚îÇ   ‚îî‚îÄ‚îÄ comprehensive_product_export.py    # Product export utility
‚îú‚îÄ‚îÄ docs/                     # Comprehensive documentation
‚îÇ   ‚îú‚îÄ‚îÄ PRODUCTION/          # Production documentation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ POSTGRESQL_MIGRATION_PLAN.md   # Migration documentation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ARCHITECTURE.md                # Architecture docs
‚îÇ   ‚îî‚îÄ‚îÄ LETTER_DATABASE_DOCUMENTATION.md   # Database schema and API
‚îú‚îÄ‚îÄ data/                     # Data directories
‚îÇ   ‚îú‚îÄ‚îÄ input/letters/        # Input obsolescence letters
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ IBcatalogue.xlsx  # Master referential (342,229 products)
‚îÇ   ‚îú‚îÄ‚îÄ output/               # Processing results
‚îÇ   ‚îî‚îÄ‚îÄ test/documents/       # Test documents
‚îú‚îÄ‚îÄ config/                   # Configuration
‚îÇ   ‚îî‚îÄ‚îÄ config.yaml           # Main configuration
‚îî‚îÄ‚îÄ logs/                     # Processing logs
    ‚îú‚îÄ‚îÄ production_pipeline.log    # Production logs
    ‚îî‚îÄ‚îÄ production_errors.log      # Error logs
```

## üîß Development and Testing

### Running Production Tests
```bash
# Test production pipeline with PostgreSQL
python scripts/production_pipeline_runner.py data/test/documents/PIX2B_Phase_out_Letter.pdf

# Test web application
cd webapp && npm run dev

# Test API endpoints
curl -X GET http://localhost:3000pi/pipeline/status

# Test database connectivity
psql -U ahuther -h localhost -d se_letters -c "SELECT COUNT(*) FROM letters;"
```

### Database Management
```bash
# Check database status
psql -U ahuther -h localhost -d se_letters -c SELECT version();"

# View processing statistics
psql -U ahuther -h localhost -d se_letters -c "
SELECT 
    COUNT(*) as total_letters,
    COUNT(CASE WHEN status = processed' THEN 1 END) as processed,
    AVG(extraction_confidence) as avg_confidence
FROM letters;"

# Backup database
pg_dump -U ahuther -h localhost se_letters > backup_$(date +%Y%m%d_%H%M%S).sql
```

### Quality Assurance
```bash
# Run integration tests
pytest tests/integration/ --disable-warnings -v

# Run unit tests
pytest tests/unit/ --disable-warnings -v

# Check code quality
pre-commit run --all-files
```

## üöÄ Migration from DuckDB to PostgreSQL

The application has been successfully migrated from DuckDB to PostgreSQL for enhanced production capabilities:

### Migration Benefits
- **Concurrent Access**: Eliminates database lock conflicts
- **Enterprise Scalability**: Supports multiple users and high-volume processing
- **ACID Compliance**: Full transaction support with rollback capabilities
- **Connection Pooling**: Optimized database connections
- **Production Ready**: Enterprise-grade database for production deployment

### Migration Status
- ‚úÖ **Schema Migration**: Complete PostgreSQL schema with all tables and indexes
- ‚úÖ **Data Migration**: All existing data migrated with 100% integrity
- ‚úÖ **Service Updates**: All services updated to use PostgreSQL
- ‚úÖ **Webapp Integration**: Web application fully integrated with PostgreSQL
- ‚úÖ **Testing**: Comprehensive integration tests passing

### Migration Commands
```bash
# Create PostgreSQL database
sudo -u postgres createdb se_letters

# Run schema migration
psql -U ahuther -h localhost -d se_letters -f scripts/migrate_schema_to_postgresql.sql

# Verify migration
psql -U ahuther -h localhost -d se_letters -c "SELECT COUNT(*) FROM letters;"
```

## üìà Performance Improvements

### PostgreSQL Optimizations
- **Connection Pooling**: 20 concurrent connections for high throughput
- **Indexed Queries**: Optimized indexes for fast letter and product searches
- **Transaction Management**: Proper commit/rollback for data integrity
- **Query Optimization**: Efficient SQL queries for large datasets

### Processing Enhancements
- **Parallel Processing**: Concurrent document processing capabilities
- **Memory Optimization**: Efficient handling of large IBcatalogue dataset
- **Error Recovery**: Robust error handling with automatic retry logic
- **Progress Tracking**: Real-time processing status updates

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature`)
4.Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üÜò Support

For support and questions:
- Create an issue on GitHub
- Check the documentation in the `docs/` directory
- Review the PostgreSQL migration documentation

---

**Built with ‚ù§Ô∏è for Schneider Electric by Alexandre Huther**